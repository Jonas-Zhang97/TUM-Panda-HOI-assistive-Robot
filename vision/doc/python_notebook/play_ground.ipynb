{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Check\n",
    "\n",
    "This python note book contains the functionality to check the data saved for or generated by the contact graspnet, to figure out how can I use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tf.transformations import quaternion_from_matrix as matrix2quaternion\n",
    "from geometry_msgs.msg import PoseStamped\n",
    "from tf.transformations import quaternion_from_euler as rpy2quaternion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the Input Data\n",
    "\n",
    "### What should be saved\n",
    "\n",
    "Here the input data of the example file provided by the repo and what I saved to .npy file are compared to verify if the saved data is correct\n",
    "\n",
    "<img src=\"https://i.kym-cdn.com/photos/images/newsfeed/002/046/368/3fa\" width=\"300\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_test_data = np.load(\"/home/franka/contact_graspnet/test_data/13.npy\", allow_pickle=True)\n",
    "# print(reader_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows that the depth should be given in meter, but the encoding method of the realsense camera is 16UC1, which means the depth value is given by mm, so the saved cv::Mat should be multipied with 10^(-3)\n",
    " \n",
    "### What I saved:\n",
    "\n",
    "Depth image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_my_data_depth_image = np.load(\"/home/franka/contact_graspnet/depth_image_data/data.npy\", allow_pickle=True)\n",
    "# print(reader_my_data_depth_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_my_data_point_cloud = np.load(\"/home/franka/contact_graspnet/point_cloud_data/data.npy\", allow_pickle=True)\n",
    "# print(reader_my_data_point_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Output Data\n",
    "\n",
    "### Output of Contact GraspNet\n",
    "\n",
    "The output data are generated in a .npz file, here they are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = np.load(\"/home/franka/contact_graspnet/results/predictions_data.npz\", allow_pickle=True)\n",
    "\n",
    "pred_grasps_cam = output_data[\"pred_grasps_cam.npy\"].item()[-1]\n",
    "scores = output_data[\"scores.npy\"].item()[-1]\n",
    "contact_pts = output_data[\"contact_pts.npy\"].item()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.tenor.com/RXyf_3Ud5FAAAAAC/%E8%AE%A9%E6%88%91%E7%9C%8B%E7%9C%8B.gif\" width=\"600\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.7069809   0.5523056   0.44174266  0.41837046]\n",
      "  [ 0.4647053  -0.10806704  0.8788461   0.10710318]\n",
      "  [ 0.53312945  0.8266075  -0.18025804  1.0956975 ]\n",
      "  [ 0.          0.          0.          1.        ]]\n",
      "\n",
      " [[-0.7362327   0.5244902   0.42763484  0.4196524 ]\n",
      "  [ 0.44395885 -0.1025844   0.89015555  0.10770498]\n",
      "  [ 0.51074654  0.84521395 -0.15732609  1.090282  ]\n",
      "  [ 0.          0.          0.          1.        ]]\n",
      "\n",
      " [[-0.7107312   0.5430437   0.44717392  0.41187283]\n",
      "  [ 0.46418095 -0.11561412  0.8781625   0.10633045]\n",
      "  [ 0.52858025  0.8317071  -0.16989993  1.0836174 ]\n",
      "  [ 0.          0.          0.          1.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(pred_grasps_cam)\n",
    "# print(pred_grasps_cam.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should be the transformation between the camera_color_optical_link and the grasp pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18229796 0.1933767  0.18147434]\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "# print(scores.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46545076 0.19705293 1.076     ]\n",
      " [0.46533206 0.19886532 1.073     ]\n",
      " [0.4595222  0.19621053 1.065     ]]\n"
     ]
    }
   ],
   "source": [
    "print(contact_pts)\n",
    "# print(contact_pts.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Use the Output\n",
    "\n",
    "Now the out put is the potential grasp candidate of the selected object, just to pick the most seccessful one and calculate cooresponding geometry_msgs::PoseStamped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7362327   0.5244902   0.42763484  0.4196524 ]\n",
      " [ 0.44395885 -0.1025844   0.89015555  0.10770498]\n",
      " [ 0.51074654  0.84521395 -0.15732609  1.090282  ]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "index_max_score = np.argmax(scores)\n",
    "# print(index_max_score)\n",
    "T_grasp_cam = np.array(pred_grasps_cam[index_max_score])\n",
    "print(T_grasp_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation\n",
      "[[-0.7362327   0.5244902   0.42763484]\n",
      " [ 0.44395885 -0.1025844   0.89015555]\n",
      " [ 0.51074654  0.84521395 -0.15732609]]\n",
      "\n",
      "Translation\n",
      "[0.4196524  0.10770498 1.090282  ]\n"
     ]
    }
   ],
   "source": [
    "r_cam_grasp = T_grasp_cam[0:3, 0:3]\n",
    "t_cam_grasp = T_grasp_cam[0:3, 3]\n",
    "print(\"Rotation\")\n",
    "print(r_cam_grasp)\n",
    "print(\"\")\n",
    "print(\"Translation\")\n",
    "print(t_cam_grasp)\n",
    "\n",
    "T_cam_base = np.array([[-0.73383682, -0.27496601,  0.62119016,  0.09599208],\n",
    "                       [-0.67863248,  0.3380292,  -0.65206919,  0.8253307 ],\n",
    "                       [-0.03068355, -0.90007219, -0.43465914,  0.73996789],\n",
    "                       [ 0.,          0.,          0.,          1.        ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take translation as a point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_grasp_base = np.dot(T_grasp_cam, T_cam_base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
